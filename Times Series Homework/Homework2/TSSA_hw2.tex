\documentclass[11pt]{article}

\usepackage{amsmath,amsthm,amssymb}

%%%%% Matrix stretcher
% use it as:
%\begin{pmatrix}[1.5]
% ...
\makeatletter
\renewcommand*\env@matrix[1][\arraystretch]{%
  \edef\arraystretch{#1}%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols c}}
\makeatother
%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\extrafootertext[1]{%
    \bgroup
    \renewcommand\thefootnote{\fnsymbol{footnote}}%
    \renewcommand\thempfootnote{\fnsymbol{mpfootnote}}%
    \footnotetext[0]{#1}%
    \egroup
}


%%%%%%%%%%%%% Colors %%%%%%%%%%%%%
\usepackage[dvipsnames]{xcolor}

\definecolor{C0}{HTML}{1d1d1d}
\definecolor{C1}{HTML}{1e3668}
\definecolor{C2}{HTML}{199d8b}
\definecolor{C3}{HTML}{d52f4c}
\definecolor{C4}{HTML}{5ab2d6}
\definecolor{C5}{HTML}{ffb268}
\definecolor{C6}{HTML}{ff7300} % for commenting - {fire orange}dd571c
\definecolor{C7}{HTML}{777b7e} % for remarks - {steel grey}
\color{C0}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%% Fonts %%%%%%%%%%%%% 
%\usepackage{fontspec}
\usepackage[no-math]{fontspec} % for text

\emergencystretch=8pt
\hyphenpenalty=1000 % default 50
\tolerance=800      % default 200
%\righthyphenmin=4
%\lefthyphenmin=4

%%% Text Font: Vollkorn + Math Font: Latin Modern (default) %%%
\setmainfont{Vollkorn}[
UprightFont = Vollkorn-Regular,
ItalicFont =Vollkorn-Italic, 
BoldItalicFont={Vollkorn-BoldItalic},
BoldFont = Vollkorn-Bold,
RawFeature=+lnum,
WordSpace=1.7,
] 

%%% We need this for math font packages other than latin modern %%%
% \usepackage{unicode-math}        % for math

%%% Text Font: Palatino + Math Font: Asana-Math %%%
%\setmainfont{Palatino}[
%BoldFont = Palatino-Bold,
%ItalicFont = Palatino-Italic,
%BoldItalicFont={Palatino-BoldItalic},
%RawFeature=+lnum,
%WordSpace=1.7,
%]
%\setmathfont{asana-math}

%%% Text Font: Arno Pro + Math Font: Minion Pro %%%
%\setmainfont{Arno Pro}[
%UprightFont = *-Regular,
%ItalicFont = Vollkorn-Italic, 
%BoldItalicFont={*-BoldItalic},
%BoldFont = *-Bold,
%RawFeature=+lnum,
%WordSpace=1.7,
%Scale= 1.1
%] 
% Minion Pro is too expensive

%%% Math Fonts %%%
%\setmathfont{Vollkorn}
%\setmathfont{Latin Modern Math}
%\setmathfont{TeX Gyre Pagella Math}
%\setmathfont{TeX Gyre Termes Math}
%\setmathfont{TeX Gyre DejaVu Math}
%\setmathfont[Scale=MatchLowercase]{DejaVu Math TeX Gyre}
%\setmathfont{XITS Math}
%\setmathfont{Libertinus Math}
%\setmathfont[Scale=MatchUppercase]{Asana Math}
%\setmathfont{STIX Two Math}

%\usepackage{kpfonts-otf}
%\setmathfont{KpMath-Regular.otf}[version=regular]
%\setmathfont{KpMath-Bold.otf}[version=bold]
%\setmathfont{KpMath-Semibold.otf}[version=semibold]
%\setmathfont{KpMath-Sans.otf}[version=sans]
%\setmathfont{KpMath-Light.otf}[version=light]


%%% CJK Fonts %%%
\usepackage[scale=.78]{luatexja-fontspec}
\setmainjfont{BabelStone Han}[AutoFakeBold]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% This package simplifies the insertion of external multi-page PDF or PS documents.
\usepackage{pdfpages}

% cref
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=C4,
    filecolor=magenta,      
    urlcolor=cyan,
    }

\usepackage[nameinlink,noabbrev,capitalize]{cleveref}
% \crefname{ineq}{}{}
% \crefname{equation}{}{}
% \creflabelformat{ineq}{#2{\textup{(1)}}#3}
% \creflabelformat{equation}{#2\textup{(#1)}#3}

%%%%%%%%%%%%% Environments %%%%%%%%%%%%%%%%
%amsthm has three separate predefined styles:	
%
%\theoremstyle{plain} is the default. it sets the text in italic and adds extra space above and below the \newtheorems listed below it in the input. it is recommended for theorems, corollaries, lemmas, propositions, conjectures, criteria, and (possibly; depends on the subject area) algorithms.
%
%\theoremstyle{definition} adds extra space above and below, but sets the text in roman. it is recommended for definitions, conditions, problems, and examples; i've alse seen it used for exercises.
%
%\theoremstyle{remark} is set in roman, with no additional space above or below. it is recommended for remarks, notes, notation, claims, summaries, acknowledgments, cases, and conclusions.

%%%  theorem-like environment %%%
\theoremstyle{plain} % default theorem style
\newtheorem{theorem}{Theorem}[section]
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{property}[theorem]{Property}

\newtheorem{definition}[theorem]{Definition}

%%% definition-like environment %%%
%\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{problem}[theorem]{Problem}


%%% framed package is great %%%
\usepackage{framed}
\newenvironment{solution}
{\color{C2}\normalfont\begin{framed}\begingroup\textbf{Solution:} }
  {\endgroup\end{framed}}

\newtheoremstyle{remark}% name of the style to be used
  {}% measure of space to leave above the theorem. E.g.: 3pt
  {}% measure of space to leave below the theorem. E.g.: 3pt
  {\color{C3}}% name of font to use in the body of the theorem
  {}% measure of space to indent
  {\color{C3}\bfseries}% name of head font
  {.}% punctuation between head and body
  { }% space after theorem head; " " = normal interword space
  {}
\theoremstyle{remark}
\newtheorem{remarkx}[theorem]{Remark}
\newenvironment{remark}
  {\pushQED{\qed}\renewcommand{\qedsymbol}{$\triangle$}\remarkx}
  {\popQED\endremarkx}
  
\newenvironment{point}
  {\O~~}
  {}

\usepackage{thmtools}
\usepackage{thm-restate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% This package is for the long equal sign \xlongequal{}
\usepackage{extarrows}


% Page Formatting
\usepackage[
    paper=a3paper,
    inner=22mm,         % Inner margin
    outer=22mm,         % Outer margin
    bindingoffset=0mm, % Binding offset
    top=28mm,           % Top margin
    bottom=22mm,        % Bottom margin
    %showframe,         % show how the type block is set on the page
]{geometry}

\setlength{\parindent}{0em}
\setlength{\parskip}{.7em}


\usepackage{tikz}
\usepackage{graphicx}
\usepackage{enumitem}
\setlist{topsep=0pt}

\usepackage{bm}

\usepackage[font=scriptsize,labelfont=bf]{caption}
\usepackage{listings}
\lstset{basicstyle=\ttfamily,breaklines=true}
% \setlength{\parskip}{1em}
% \setlength{\parindent}{0em}
\usepackage{dsfont}
\newcommand{\bOne}{\mathds{1}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\VV}{\mathbb{V}}
\newcommand{\CoV}{\operatorname{Co\mathbb{V}}}

% header
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{\small   \bfseries Homework}
\fancyhead[C]{\small   \bfseries Fall 2023}
\fancyhead[R]{\small   \bfseries Zhou}


\begin{document}

\begin{center}
  \text{\Large{Time Series Foundamentals
    }}

  {\text{Kaiwen Zhou}}
\end{center}
\vspace{2em}

\tableofcontents

\section{Topic: Mean Squared Prediction}

Suppose $\left\{X_t\right\}$ is a stationary process with mean $\mu$, and auto-covariance function $\gamma_X(h)$.
Our goal is to predict: $X_{n+h}$ using observations $X_1, X_2, \ldots, X_n$.

We define the \textbf{Best Linear Predictor} to be the one minimizes the Mean Squared Prediction Error (MSPE):
$$
  \widehat{X}_{n+h}=E\left[X_{n+h} \mid X_1, \ldots, X_n\right] = P_n X_{n+h} : =a_0^*+a_1^* X_n+ \cdots +a_n^* X_1 = a_0 + \mathbf{a}_n^\top \mathbf{x}_n = \arg\min_{a_0, \mathbf{a}_n} \EE\left[\left(X_{n+h}-a_0 - \mathbf{a}_n^\top \mathbf{x}_n\right)^2\right] = \arg\min_{a_0, \mathbf{a}_n} L(a_0, \mathbf{a}_n)
$$
where $\mathbf{a}_n = (a_1, \ldots, a_n) \in\mathbb{R}^n$, $\mathbf{x}_n = (x_n, \ldots, x_1)\in\mathbb{R}^n$, $P$ in $P_n$ stands for ``Predictor'' and $n$ suggests that we are using $n$ most recent observations.

To simplify our computation, we define $\bOne = (1,\ldots, 1) \in\mathbb{R}^n$,
$\bm{\gamma}_n(h) = \begin{bmatrix}\gamma_X(h)   \\
    \gamma_X(h+1) \\
    \vdots        \\
    \gamma_X(h+n-1)\end{bmatrix}\in\mathbb{R}^n$,
$\Gamma_n = \begin{bmatrix}
    \gamma_X(0)   & \gamma_X(1)   & \cdots & \gamma_X(n-1) \\
    \gamma_X(1)   & \gamma_X(0)   & \cdots & \gamma_X(n-2) \\
    \vdots        & \vdots        & \ddots & \vdots        \\
    \gamma_X(n-1) & \gamma_X(n-2) & \cdots & \gamma_X(0)
  \end{bmatrix}\in\mathbb{R}^{n\times n}$.

Taking the partial derivatives of $L$, we obtain
$$
  0 = \frac{\partial L}{\partial a_0}
  = \EE\left[\frac{\partial }{\partial a_0}(X_{n+h}-a_0-\mathbf{a}_n^\top \mathbf{x}_n)^2\right]
  = \EE\left[-2(X_{n+h}-a_0-\mathbf{a}_n^\top \mathbf{x}_n)\right] = 2(a_0-\mu+\mathbf{a}_n^\top\bOne)
  \Longrightarrow a_0^* = \mu(1-\mathbf{a}_n^\top\bOne)
$$
and
\begin{align*}
  \bm{0} = \frac{\partial L}{\partial \mathbf{a}_n} & = \frac{\partial }{\partial \mathbf{a}_n}\EE\left[(X_{n+h} - a_0 - \mathbf{a}_n^\top \mathbf{x}_n)^2\right]                                                                         \\
                                                    & =\EE\left[\frac{\partial }{\partial \mathbf{a}_n}\left(\mathbf{a}_n^\top\mathbf{x}_n \mathbf{x}_n^\top\mathbf{a}_n - 2X_{n+h}\mathbf{x}_n^\top \mathbf{a}_n + 2a_0\mathbf{x}_n^\top
  \mathbf{a}_n + (\text{ terms without } \mathbf{a}_n)\right)\right]                                                                                                                                                                      \\
                                                    & =2 \EE[\mathbf{x}_n \mathbf{x}_n^\top\mathbf{a}_n - X_{n+h}\mathbf{x}_n + a_0\mathbf{x}_n]                                                                                          \\
                                                    & =2 \EE[\mathbf{x}_n \mathbf{x}_n^\top]\mathbf{a}_n - 2\EE[X_{n+h}\mathbf{x}_n] + 2\EE[a_0\mathbf{x}_n]                                                                              \\
                                                    & =2 (\Gamma_n +\mu^2 \bOne\bOne^\top)\mathbf{a}_n - 2(\bm{\gamma}_n(h) + \mu^2 \bOne) + 2a_0\mu \bOne                                                                                \\
  a_0 = \mu(1-\mathbf{a}_n^\top\bOne) \Longrightarrow
                                                    & =2 (\Gamma_n +\mu^2 \bOne\bOne^\top)\mathbf{a}_n - 2(\bm{\gamma}_n(h) + \mu^2 \bOne) + 2 \mu(1-\mathbf{a}_n^\top\bOne)\mu \bOne                                                     \\
                                                    & =2 \Gamma_n \mathbf{a}_n - 2\bm{\gamma}_n(h)                                                                                                                                        \\
  \Longrightarrow \mathbf{a}_n^*                    &
  = \Gamma_n^{-1}\bm{\gamma}_n(h)
\end{align*}
Therefore, the \textbf{Best Linear Predictor} is given by
\begin{align}
  P_n X_{n+h} : = a_0^* + \mathbf{a}_n^{*^\top} \mathbf{x}_n \text{ where }
  \begin{cases}
    a_0^*          & = \mu(1-\mathbf{a}_n^{*^\top}\bOne) \\
    \mathbf{a}_n^* & = \Gamma_n^{-1}\bm{\gamma}_n(h)
  \end{cases} \label{eq: Best Linear Predictor}
\end{align}


\begin{problem}
Show that Mean Squared Predictive Error is obtained by
$$
  \EE\left[\left(X_{n+h}-P_{n} X_{n+h}\right)^{2}\right] = \gamma_{X}(0)- \mathbf{a}_n^{*^\top}\bm{\gamma}_n(h)
$$
\end{problem}
\begin{solution}
  From above, we get
  \begin{align*}
    \EE\left[\left(X_{n+h}-P_{n} X_{n+h}\right)^{2}\right]
     & = \EE\left[\left(X_{n+h}-a_0^* -\mathbf{a}_n^{*^\top}\mathbf{x}_n\right)^{2}\right]                                                                                                                                                                                                                       \\
    \mathbf{a}_n^{*^\top}\mathbf{x}_n = \mathbf{x}_n^\top \mathbf{a}_n^* \Longrightarrow
     & = \EE\left[X_{n+h}^2 +a_0^{*^2} + \mathbf{a}_n^{*^\top}\mathbf{x}_n\mathbf{x}_n^\top\mathbf{a}_n^{*} - 2a_0^*X_{n+h} - 2X_{n+h}\mathbf{a}_n^{*^\top}\mathbf{x}_n + 2a_0^*\mathbf{a}_n^{*^\top}\mathbf{x}_n\right]                                                                                         \\
     & = \EE\left[X_{n+h}^2\right] + a_0^{*^2} + \mathbf{a}_n^{*^\top}\EE[\mathbf{x}_n\mathbf{x}_n^\top]\mathbf{a}_n^{*} - 2a_0^*\EE[X_{n+h}] - 2\mathbf{a}_n^{*^\top}\EE[X_{n+h}\mathbf{x}_n] + 2a_0^*\mathbf{a}_n^{*^\top}\EE\left[\mathbf{x}_n\right]                                                         \\
     & = \gamma_{X}(0) + \mu^2 + \mu^2(1-\mathbf{a}_n^{*^\top}\bOne)^2 + \mathbf{a}_n^{*^\top}(\Gamma_n +\mu^2 \bOne\bOne^\top)\mathbf{a}_n^{*} - 2\mu^2(1-\mathbf{a}_n^{*^\top}\bOne) - 2\mathbf{a}_n^{*^\top}(\bm{\gamma}_n(h) + \mu^2 \bOne) + 2\mu^2(1-\mathbf{a}_n^{*^\top}\bOne)\mathbf{a}_n^{*^\top}\bOne \\
     & = \gamma_{X}(0) + \mathbf{a}_n^{*^\top}\Gamma_n\mathbf{a}_n^{*} - 2\mathbf{a}_n^{*^\top}\bm{\gamma}_n(h)                                                                                                                                                                                                  \\
    \mathbf{a}_n^* = \Gamma_n^{-1}\bm{\gamma}_n(h) \Longrightarrow
     & = \gamma_{X}(0) + \mathbf{a}_n^{*^\top}\Gamma_n\Gamma_n^{-1}\bm{\gamma}_n(h) - 2\mathbf{a}_n^{*^\top}\bm{\gamma}_n(h)                                                                                                                                                                                     \\
     & = \gamma_{X}(0)- \mathbf{a}_n^{*^\top}\bm{\gamma}_n(h)
  \end{align*}
  Hence, we can conclude that the Mean Squared Predictive Error is obtained by
  $$
    \EE\left[\left(X_{n+h}-P_{n} X_{n+h}\right)^{2}\right] = \gamma_{X}(0)- \mathbf{a}_n^{*^\top}\bm{\gamma}_n(h)
  $$
\end{solution}

\begin{remark}\hfill
  \begin{enumerate}
    \item For a linear Gaussian process $\left\{X_{t}\right\}$,
          take $X_{n+h} \mid X_1, \ldots, X_n = P_{n} X_{n+h} + \epsilon = a_0^* + \mathbf{a}_n^{*^\top}\mathbf{x}_n + \epsilon$, then the residual term $\epsilon$ must be Gaussian.
          Then, we have
          $$
            \EE[X_{n+h} \mid X_1, \ldots, X_n - P_{n} X_{n+h}] = \mu - a_0^* + \mu\mathbf{a}_n^{*^\top}\bOne = 0
            \xrightarrow{a_0^*= \mu(1-\mathbf{a}_n^{*^\top}\bOne)}
            \epsilon\sim \mathcal{N}(0, \operatorname{MSPE}(P_n X_{n+h})) = \mathcal{N}(0, \gamma_{X}(0)- \mathbf{a}_n^{*^\top}\bm{\gamma}_n(h))
          $$
          by our computation; thus, the predictive distribution is also Gaussian:
          $$
            X_{n+h} \mid X_1, \ldots, X_n \sim \mathcal{N}\left(P_n X_{n+h}, \operatorname{MSPE}\left(P_n X_{n+h}\right)\right) = \mathcal{N}\left(a_0^* + \mathbf{a}_n^{*^\top}\mathbf{x}_n, \gamma_{X}(0)- \mathbf{a}_n^{*^\top}\bm{\gamma}_n(h)\right)
          $$
          where $a_0^* = \mu(1-\mathbf{a}_n^{*^\top}\bOne)$ and $\mathbf{a}_n^* = \Gamma_n^{-1}\bm{\gamma}_n(h)$.
    \item For large $n, \Gamma_n^{-1}$ is obtained through recursive algorithms. As an example look at Durbin-Levinson Algorithm
  \end{enumerate}
\end{remark}



\section{Topic: MLE for AR and MA processes}
\subsection*{Direct MLE}
Suppose that $\left\{X_t\right\}$ is a Gaussian time series (process) with mean $\mu$ and autocovariance function $\gamma_X(i, j)$.
Let $\mathbf{x}_n=\left(X_1, \ldots, X_n\right)\in \mathbb{R}^n$.
Let $\Gamma_n$ denote the covariance matrix $\Gamma_n=Cov\left(\mathbf{x}_n, \mathbf{x}_n\right)$, and assume that $\Gamma_n$ is nonsingular.
Since any subset of a Gaussian process are jointly Gaussian, the likelihood function for given observation $\mathbf{x}_n$ (a single obseravtion) is
$$
  L\left(\Psi\right)= f(\mathbf{x}_n \mid \Psi) = \frac{1}{\sqrt{(2 \pi)^n|\Gamma_n|}} \exp \left(-\frac{1}{2} (\mathbf{x}_n-\mu)^\top \Gamma_n^{-1} (\mathbf{x}_n-\mu)\right) .
$$
where $\Psi$ is the parameter set, e.g.for the ARMA model, $\Psi_{ARMA(p, q)}=\left(\phi_1, \ldots, \phi_{p}, \theta_1, \ldots, \theta_{q}, \sigma^2\right)$.

To simplify our computation, we could consider the log-likelihood function:
$$
  \log L(\Psi)= -\frac{n}{2} \log (2 \pi)+\frac{1}{2} \log \left|\Gamma_n\right|^{-1}-\frac{1}{2}(\mathbf{x}_n-\boldsymbol{\mu})^\top \Gamma_n^{-1}(\mathbf{x}_n-\boldsymbol{\mu})
$$

Then, the values of parameters $\Psi$ which maximizes the Log-likelihood function is the Maximum Likelihood Estimate of $\Psi$. That is,
$$
  \hat{\Psi}_{MLE}=\arg \max_{\Psi} \log L(\Psi)
$$
\subsection*{Conditional MLE}
For the ease of implementation, we can also use the conditional MLE. Using the fact that
$$
  f(\mathbf{x}_n \mid \Psi) = f(x_1, \ldots, x_n \mid \Psi)
  = f(x_n \mid x_1, \ldots, x_{n-1}, \Psi) f(x_1, \ldots, x_{n-1} \mid \Psi)
  \xlongequal{\cdots} f(x_n \mid x_1, \ldots, x_{n-1}, \Psi)f(x_{n-1} \mid x_1, \ldots, x_{n-2}, \Psi) \cdots f(x_1 \mid \Psi)
$$
Then, the MLE is given by
\begin{align}
  \log L(\Psi) = \log f(x_1 \mid \Psi) + \sum_{i=2}^{n} \log f(x_{i} \mid x_1, \ldots, x_{i-1}, \Psi) \label{eq:conditional MLE}
\end{align}
Similarly, the values of parameters $\Psi$ which maximizes the Log-likelihood function is the Maximum Likelihood Estimate of $\Psi$. That is,
$$
  \hat{\Psi}_{MLE}=\arg \max_{\Psi} \log L(\Psi)
$$

\begin{remark}\hfill
  \begin{enumerate}
    \item We mostly use Conditional MLE in applications.
    \item Since $X_1$ is the the first data point, the way it's picked is on its own. If $X_1$ is pick based on some distribution, we can surely keep the term $\log f(x_1\mid \Psi)$ in the above equation
          and work with the exact MLE. If how $X_1$ is picked is unknown, we can treat it as deterministic, and drop the term $\log f(x_1\mid \Psi)$ in the above equation.
  \end{enumerate}

\end{remark}

\begin{problem}
Using a programming language of your choice, repeat the MLE construction and simulation for AR and MA models as discussed this in the class.
\end{problem}

\begin{solution}
  \begin{enumerate}[label = (\alph*)]
    \item \textbf{Conditional MLE for AR(1) Process}

          Consider observations $\left\{X_1, X_2, \ldots, X_n\right\}$ of a stationary Gaussian
          AR(1) process. For parameter vector is $\Psi=\left(\phi_0, \phi, \sigma^2\right)$, we have
          $$
            X_t=\phi_0+\phi X_{t-1}+Z_t,  Z_t \sim IIDN\left(0, \sigma^2\right), \quad
            \EE[X_t]=\frac{\phi_0}{1-\phi}, Var\left(X_t\right)=\frac{\sigma^2}{1-\phi^2} \text{ and } \gamma_X(h)=\frac{\phi^h \sigma^2}{1-\phi^2}
          $$
          The probability distribution for $X_1$ can be written as,
          $$
            X_1 \sim \mathcal{N}\left(\frac{\phi_0}{1-\phi}, \frac{\sigma^2}{1-\phi^2}\right)  \Longrightarrow f\left(x_1 ; \Psi\right)=\frac{1}{\sqrt{2 \pi \frac{\sigma^2}{1-\phi^2}}} \exp \left\{\frac{-\left(x_1-\frac{\phi_0}{1-\phi}\right)^2}{2 \frac{\sigma^2}{1-\phi^2}}\right\}
          $$
          Since $X_2=\phi_0+\phi X_1+Z_2$, $X_2 \mid X_1=x_1 \sim \mathcal{N}\left(\phi_0+\phi x_1, \sigma^2\right)$.
          The conditional probability distribution for $X_2$ knowing $X_1$ can be written as,
          $$f\left(x_2 \mid x_1 ; \Psi\right)=\frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left\{\frac{-\left(x_2-\phi_0-\phi x_1\right)^2}{2 \sigma^2}\right\}
          $$
          Similarly, we have $X_n \mid X_{n-1}=x_{n-1} \sim \mathcal{N}\left(\phi_0+\phi x_{n-1}, \sigma^2\right)$ and
          $$
            f\left(x_n \mid x_{n-1} ; \Psi\right)=\frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left\{\frac{-\left(x_n-\phi_0-\phi x_{n-1}\right)^2}{2 \sigma^2}\right\}
          $$
          Plug these in \cref{eq:conditional MLE}, we obtain the Log-Likelihood function:
          \begin{align*}
            \log L(\Psi) & = \log f(x_1 \mid \Psi) + \sum_{i=2}^{n} \log f(x_{i} \mid x_1, \ldots, x_{i-1}, \Psi)                                                                                                                                                                                                                       \\
                         & = -\frac{1}{2} \log (2 \pi)-\frac{1}{2} \log \left(\frac{\sigma^2}{1-\phi^2}\right)-\frac{\left(x_1-\frac{\phi_0}{1-\phi}\right)^2}{2 \frac{\sigma^2}{1-\phi^2}} + \sum_{i=2}^n - \frac{1}{2} \log (2 \pi)-\frac{1}{2} \log \left(\sigma^2\right)- \frac{\left(x_i-\phi_0-\phi x_{i-1}\right)^2}{2 \sigma^2} \\
                         & = -\frac{1}{2} \log (2 \pi)-\frac{1}{2} \log \left(\frac{\sigma^2}{1-\phi^2}\right)-\frac{\left(x_1-\frac{\phi_0}{1-\phi}\right)^2}{2 \frac{\sigma^2}{1-\phi^2}}-\frac{n-1}{2} \log (2 \pi)-\frac{n-1}{2} \log \left(\sigma^2\right)-\sum_{i=2}^n \frac{\left(x_i-\phi_0-\phi x_{i-1}\right)^2}{2 \sigma^2}
          \end{align*}

          Maximize this function to estimate the parameters. (Much easier to implement)\\

          \hrule

          \textbf{Example:} Suppose we have AR(1) process $X_t=0.9X_{t-1}+Z_t, Z_t \sim IIDN\left(0,0.7^2\right)$. We simulate this process and pretend that we do not know the true values for $\phi$ and $\sigma$.
          Then, we use the conditional MLE to estimate the parameter vector $\Psi=\left(\phi, \sigma^2\right)$
          By the above deduction, we get the log likelihood function is
          \begin{align*}
            \log L(\Psi)                                 & =-\frac{1}{2} \log (2 \pi)-\frac{1}{2} \log \left(\frac{\sigma^2}{1-\phi^2}\right)-\frac{x_1^2}{2 \frac{\sigma^2}{1-\phi^2}}-\frac{n-1}{2} \log (2 \pi)-\frac{n-1}{2} \log \left(\sigma^2\right)-\sum_{i=2}^n \frac{\left(x_i-\phi x_{i-1}\right)^2}{2 \sigma^2} \\
            \text{ treat } X_1 \text{ as deterministic } & = -\frac{n-1}{2} \log (2 \pi)-\frac{n-1}{2} \log \left(\sigma^2\right)-\sum_{i=2}^n \frac{\left(x_i-\phi x_{i-1}\right)^2}{2 \sigma^2}
          \end{align*}
          Then, we have
          $$
            \hat{\Psi}_{MLE}=\arg \max_{\Psi} \log L(\Psi)
          $$
          For the coding work on this problem, check the attached jupyter notebook \texttt{Time Series and Statistical Arbitrage HW2.ipynb}
    \item \textbf{Conditional MLE for MA(1) Process}

          Consider observations $\left\{X_1, X_2, \ldots, X_n\right\}$ of a Gaussian
          MA(1) process. For parameter vector is $\Psi=\left(\theta_0, \theta_1, \sigma^2\right)$, we have
          $$
            X_t=\theta_0+\theta_1 Z_{t-1}+Z_t, \quad Z_t \sim IIDN\left(0, \sigma^2\right)
          $$
          The probability distribution for $X_t$ can be written as,
          $$
            X_t \mid Z_{t-1} \sim \mathcal{N}\left(\theta_0+\theta_1 z_{t-1}, \sigma^2\right) \Longrightarrow  f\left(x_t \mid z_{t-1}\right)=\frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left\{\frac{-\left(x_t-\left(\theta_0+\theta_1 z_{t-1}\right)\right)^2}{2 \sigma^2}\right\}
          $$
          Plug these in \cref{eq:conditional MLE}, we obtain the Log-Likelihood function:
          \begin{align*}
            \log L(\Psi)                                 & = \log f(x_1 \mid \Psi) + \sum_{i=2}^{n} \log f(x_{i} \mid x_1, \ldots, x_{i-1}, \Psi)                                                              \\
            \text{ treat } X_1 \text{ as deterministic } & = -\frac{n-1}{2} \log (2 \pi)-\frac{n-1}{2} \log \left(\sigma^2\right)-\sum_{i=2}^n \frac{\left(x_i-\theta_0-\theta_1 z_{i-1}\right)^2}{2 \sigma^2}
          \end{align*}

          Maximize this function to estimate the parameters. (Much easier to implement)\\

          \hrule

          \textbf{Example:} Suppose we have MA(1) process $X_t=0.5Z_{t-1}+Z_t, Z_t \sim IIDN\left(0,0.5^2\right)$. We simulate this process and pretend that we do not know the true values for $\phi$ and $\sigma$.
          Then, we use the conditional MLE to estimate the parameter vector $\Psi=\left(\theta_1, \sigma^2\right)$
          By the above deduction, we get the log likelihood function is
          $$
            \log L(\Psi)=-\frac{n-1}{2} \log (2 \pi)-\frac{n-1}{2} \log \left(\sigma^2\right)-\sum_{i=2}^n \frac{\left(x_i-\theta_1 z_{i-1}\right)^2}{2 \sigma^2}
          $$
          Then, we have
          $$
            \hat{\Psi}_{MLE}=\arg \max_{\Psi} \log L(\Psi)
          $$
          For the coding work on this problem, check the attached jupyter notebook \texttt{Time Series and Statistical Arbitrage HW2.ipynb}
  \end{enumerate}


\end{solution}

\section{Topic: MLE for AR and MA processes}
\begin{problem}
For the time series model below:
$$
  X_{t}=0.9 X_{t-1}+Z_{t} \quad Z_{t} \text{ is } N\left(0,0.7^{2}\right)
$$

\begin{enumerate}[label = (\Roman*)]
  \item Simulate a path with 1000 data points.
  \item Take the simulated path as the realized data and estimate the parameters using the MLE estimator.
  \item Repeat steps (I) and (II) for 100 times and plot the distributions of the parameters you have estimated. What are the $95 \%$ confidence levels around your mean estimated parameters?
  \item Repeat (III) but use 5000 times instead of 100 times.
\end{enumerate}

Use any programming language or algorithm you are comfortable with but we need to see the code.

\end{problem}
\begin{solution}
  Please check the attached jupyter notebook \texttt{Time Series and Statistical Arbitrage HW2.ipynb}
\end{solution}


\section{Application: Mean Squared Prediction and the Best Linear Predictor}
\begin{problem}
An MA(1) process is given by
$$
  X_{t}=Z_{t}+\theta Z_{t-1} \quad \theta=0.8, \quad Z_{t} \sim W N(0,1)
$$

Our observations indicates that $\left\{X_{1}=3.2020, X_{2}=1.5625\right\}$.

What is the best linear prediction and MSPE for $X_{3}$?
\end{problem}

\begin{solution}
  From \cref{eq: Best Linear Predictor}, we know that
  $$
    P_n X_{n+h} : = a_0^* + \mathbf{a}_n^{*^\top} \mathbf{x}_n \text{ where }
    \begin{cases}
      a_0^*          & = \mu(1-\mathbf{a}_n^{*^\top}\bOne) \\
      \mathbf{a}_n^* & = \Gamma_n^{-1}\bm{\gamma}_n(h)
    \end{cases}
    \quad \xrightarrow{n=2, h=1} \quad
    P_2 X_{3} : = a_0^* + \mathbf{a}_2^{*^\top} \mathbf{x}_2 \text{ where }
    \begin{cases}
      a_0^*          & = \mu(1-\mathbf{a}_2^{*^\top}\bOne) \\
      \mathbf{a}_2^* & = \Gamma_2^{-1}\bm{\gamma}_2(1)
    \end{cases}
  $$
  Since $\EE[X_t] = 0 = \mu$ and $\gamma_X(h) = (1+\theta^2)\gamma_Z(h) + \theta\gamma_Z(h-1) + \theta\gamma_Z(h+1) = \begin{cases}
      (1+\theta^2)\sigma^2 & \text{ if } h = 0     \\
      \theta\sigma^2       & \text{ if } h = \pm 1 \\
      0                    & \text{ otherwise }
    \end{cases}$, we have
  $$
    \Gamma_2 = \begin{bmatrix}
      \gamma_X(0) & \gamma_X(1) \\
      \gamma_X(1) & \gamma_X(0)
    \end{bmatrix} = \begin{bmatrix}
      (1+\theta^2)\sigma^2 & \theta\sigma^2       \\
      \theta\sigma^2       & (1+\theta^2)\sigma^2
    \end{bmatrix},
    \bm{\gamma}_2(1) = \begin{bmatrix}
      \gamma_X(1) \\
      \gamma_X(2)
    \end{bmatrix} = \begin{bmatrix}
      \theta\sigma^2 \\
      0
    \end{bmatrix}
    \Longrightarrow
    \begin{cases}
      a_0^*          & = 0                             \\
      \mathbf{a}_2^* & = \Gamma_2^{-1}\bm{\gamma}_2(1)
      = \frac{1}{1+\theta^2 + \theta^4}
      \begin{bmatrix}
        \theta + \theta^3 \\
        -\theta^2
      \end{bmatrix} \xlongequal{\theta = 0.8}
      \begin{bmatrix}
        \frac{820}{1281} \\
        -\frac{400}{1281}
      \end{bmatrix}
    \end{cases}
  $$
  Therefore, the Best Linear Predictor for $X_3$ is
  $$
    \hat{X}_3 = P_n X_{n+h} : = a_0^* + \mathbf{a}_n^{*^\top} \mathbf{x}_n = \frac{820}{1281}X_2 -\frac{400}{1281}X_1 = 0.0003512880562062115
  $$

  The corresponding MSPE is
  $$
    MSPE = \EE\left[\left(X_{n+h}-P_{n} X_{n+h}\right)^{2}\right] = \gamma_{X}(0)- \mathbf{a}_n^{*^\top}\bm{\gamma}_n(h)
    \xrightarrow{n=2, h=1} =\gamma_{X}(0)- \mathbf{a}_2^{*^\top}\bm{\gamma}_2(1) = 1.64 - \begin{bmatrix}
      \frac{820}{1281} & -\frac{400}{1281}
    \end{bmatrix}\begin{bmatrix}
      0.8 \\
      0
    \end{bmatrix} = 1.1279000780640125
  $$

  Hence, the Best Linear Predictor for $X_3$ is $\hat{X}_3= 0.0003512880562062115$ and the corresponding $MSPE = 1.1279000780640125$.
\end{solution}


\end{document}

